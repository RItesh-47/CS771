{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yUyCLlWooZd"
      },
      "source": [
        "Direct mean update : means were updated directly in an online manner without the use of bayesian statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "# Classifier using direct class mean update\n",
        "class MeanUpdateClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(MeanUpdateClassifier, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.input_dim = input_dim\n",
        "        self.class_means = nn.Parameter(torch.zeros(num_classes, input_dim), requires_grad=False)\n",
        "\n",
        "    def update_means(self, features, labels):\n",
        "        # Compute mean feature vector for each class\n",
        "        for c in range(self.num_classes):\n",
        "            class_features = features[labels == c]\n",
        "            if len(class_features) > 0:\n",
        "                # Update class mean by averaging the features of this class\n",
        "                self.class_means.data[c] = class_features.mean(dim=0)\n",
        "\n",
        "    def forward(self, features):\n",
        "        # Normalize the features and class means (optional step)\n",
        "        normalized_features = features / (torch.norm(features, dim=1, keepdim=True) + 1e-6)\n",
        "        normalized_class_means = self.class_means / (torch.norm(self.class_means, dim=1, keepdim=True) + 1e-6)\n",
        "\n",
        "        # Compute cosine similarity between input features and class means\n",
        "        cosine_similarity = torch.mm(normalized_features, normalized_class_means.T)\n",
        "        return cosine_similarity\n",
        "\n",
        "# Main Functionality\n",
        "def train_and_evaluate():\n",
        "    train_data_path = '/content/drive/MyDrive/ML/dataset/dataset/part_one_dataset/train_data'\n",
        "    eval_data_path = '/content/drive/MyDrive/ML/dataset/dataset/part_one_dataset/eval_data'\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize for ViT input\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize for ViT\n",
        "    ])\n",
        "\n",
        "    batch_size = 64\n",
        "    confidence_threshold = 0.8\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize Feature Extractor\n",
        "    feature_extractor = ViTFeatureExtractorModel(pretrained=True).to(device)\n",
        "\n",
        "    accuracy_matrix = np.zeros((10, 10))\n",
        "\n",
        "    for i in range(1, 11):\n",
        "        print(f\"\\n=== Processing Dataset D{i} ===\")\n",
        "\n",
        "        train_dataset = CustomCIFARDataset(\n",
        "            os.path.join(train_data_path, f'{i}_train_data.tar.pth'),\n",
        "            transform=transform,\n",
        "            labeled=(i == 1)\n",
        "        )\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        features_list, labels_list = [], []\n",
        "        for batch in train_loader:\n",
        "            if i == 1:  # D1 is labeled\n",
        "                images, labels = batch\n",
        "                labels_list.append(labels.to(device))\n",
        "            else:  # D2 onward\n",
        "                images = batch\n",
        "\n",
        "            images = images.to(device)\n",
        "            features = feature_extractor(images)  # Extract features\n",
        "            features_list.append(features)\n",
        "\n",
        "        features = torch.cat(features_list).to(device)  # Combine all features\n",
        "        if labels_list:\n",
        "            labels = torch.cat(labels_list).to(device)\n",
        "            num_classes = len(torch.unique(labels))\n",
        "        else:\n",
        "            labels = None\n",
        "            num_classes = 10  # Default for unlabeled datasets\n",
        "\n",
        "        # Initialize Classifier for D1\n",
        "        if i == 1:\n",
        "            classifier = MeanUpdateClassifier(input_dim=feature_extractor.feature_dim, num_classes=num_classes).to(device)\n",
        "\n",
        "        if labels is not None:  # D1: Update class means using true labels\n",
        "            classifier.update_means(features, labels)\n",
        "        else:  # D2 onward: Pseudo-labeling and class mean update\n",
        "            with torch.no_grad():\n",
        "                logits = classifier(features)  # Predict pseudo-labels\n",
        "                predictions = logits.argmax(dim=1)\n",
        "                confidences = logits.max(dim=1).values  # Confidence scores\n",
        "\n",
        "            # Filter samples based on confidence threshold\n",
        "            high_confidence_indices = confidences > confidence_threshold\n",
        "            high_confidence_features = features[high_confidence_indices]\n",
        "            high_confidence_labels = predictions[high_confidence_indices]\n",
        "\n",
        "            # Update class means using high-confidence pseudo-labeled samples\n",
        "            if len(high_confidence_features) > 0:\n",
        "                classifier.update_means(high_confidence_features, high_confidence_labels)\n",
        "\n",
        "        print(f\"\\nEvaluating model f{i} on held-out datasets D̂1 to D̂{i}\")\n",
        "        for j in range(1, i + 1):\n",
        "            eval_dataset = CustomCIFARDataset(\n",
        "                os.path.join(eval_data_path, f'{j}_eval_data.tar.pth'),\n",
        "                transform=transform,\n",
        "                labeled=True\n",
        "            )\n",
        "            eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            correct, total = 0, 0\n",
        "            with torch.no_grad():\n",
        "                for images, labels in eval_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    features = feature_extractor(images)\n",
        "                    logits = classifier(features)\n",
        "                    predictions = logits.argmax(dim=1)\n",
        "                    correct += (predictions == labels).sum().item()\n",
        "                    total += labels.size(0)\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            accuracy_matrix[i - 1, j - 1] = accuracy\n",
        "            print(f\"Accuracy of model f{i} on D̂{j}: {accuracy:.2f}%\")\n",
        "\n",
        "    print(\"\\nAccuracy Matrix:\")\n",
        "    for row in accuracy_matrix:\n",
        "        print(\" \".join(f\"{val:6.2f}\" for val in row))\n",
        "\n",
        "# Run\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ],
      "metadata": {
        "id": "2RH0N45dW_G-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdIwkdkj1Vsf"
      },
      "source": [
        "bayesian approach: we use a previous class means to determine labels for the current dataset and then as per the confidence levels of the label predicted, we decide whether to udpate means using it or not"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zILkooGo1XYT",
        "outputId": "f3ee0d29-ea8c-42a4-8d17-f9c143d8bbe1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 64.8MB/s]\n",
            "<ipython-input-1-b9e75adaffa4>:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data_dict = torch.load(file_path)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Processing Dataset D1 ===\n",
            "\n",
            "Evaluating model f1 on held-out datasets D̂1 to D̂1\n",
            "Accuracy of model f1 on D̂1: 32.72%\n",
            "\n",
            "=== Processing Dataset D2 ===\n",
            "\n",
            "Evaluating model f2 on held-out datasets D̂1 to D̂2\n",
            "Accuracy of model f2 on D̂1: 31.28%\n",
            "Accuracy of model f2 on D̂2: 30.40%\n",
            "\n",
            "=== Processing Dataset D3 ===\n",
            "\n",
            "Evaluating model f3 on held-out datasets D̂1 to D̂3\n",
            "Accuracy of model f3 on D̂1: 30.88%\n",
            "Accuracy of model f3 on D̂2: 30.72%\n",
            "Accuracy of model f3 on D̂3: 31.48%\n",
            "\n",
            "=== Processing Dataset D4 ===\n",
            "\n",
            "Evaluating model f4 on held-out datasets D̂1 to D̂4\n",
            "Accuracy of model f4 on D̂1: 29.24%\n",
            "Accuracy of model f4 on D̂2: 30.28%\n",
            "Accuracy of model f4 on D̂3: 31.36%\n",
            "Accuracy of model f4 on D̂4: 30.12%\n",
            "\n",
            "=== Processing Dataset D5 ===\n",
            "\n",
            "Evaluating model f5 on held-out datasets D̂1 to D̂5\n",
            "Accuracy of model f5 on D̂1: 29.40%\n",
            "Accuracy of model f5 on D̂2: 29.80%\n",
            "Accuracy of model f5 on D̂3: 30.48%\n",
            "Accuracy of model f5 on D̂4: 30.64%\n",
            "Accuracy of model f5 on D̂5: 29.76%\n",
            "\n",
            "=== Processing Dataset D6 ===\n",
            "\n",
            "Evaluating model f6 on held-out datasets D̂1 to D̂6\n",
            "Accuracy of model f6 on D̂1: 30.08%\n",
            "Accuracy of model f6 on D̂2: 28.68%\n",
            "Accuracy of model f6 on D̂3: 29.52%\n",
            "Accuracy of model f6 on D̂4: 29.48%\n",
            "Accuracy of model f6 on D̂5: 31.12%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b9e75adaffa4>\u001b[0m in \u001b[0;36m<cell line: 178>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;31m# Run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-b9e75adaffa4>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-b9e75adaffa4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Flatten the feature map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0midentity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Dataset Class\n",
        "class CustomCIFARDataset(Dataset):\n",
        "    def __init__(self, file_path, transform=None, labeled=True):\n",
        "        data_dict = torch.load(file_path)\n",
        "        self.images = data_dict['data']\n",
        "        self.labels = data_dict['targets'] if labeled else None\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        image = Image.fromarray(image)  # Convert to PIL Image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.labels is not None:\n",
        "            return image, self.labels[idx]\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "# Feature Extractor\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "        base_model = models.resnet18(pretrained=pretrained)\n",
        "        self.features = nn.Sequential(*list(base_model.children())[:-1])  # Remove the FC layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        return x.view(x.size(0), -1)  # Flatten the feature map\n",
        "\n",
        "# Bayesian Classifier\n",
        "class BayesianClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(BayesianClassifier, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.input_dim = input_dim\n",
        "        self.prior_mean = torch.zeros(num_classes, input_dim)  # Zero mean\n",
        "        self.prior_cov = torch.eye(input_dim).repeat(num_classes, 1, 1)  # Identity covariance\n",
        "\n",
        "    def update_posteriors(self, features, labels):\n",
        "        for c in range(self.num_classes):\n",
        "            class_features = features[labels == c]\n",
        "            if len(class_features) > 0:\n",
        "                n_c = len(class_features)\n",
        "                sample_mean = class_features.mean(dim=0)\n",
        "\n",
        "                prior_cov_inv = torch.linalg.inv(self.prior_cov[c])\n",
        "                likelihood_cov_inv = torch.eye(self.input_dim, device=features.device) * n_c\n",
        "\n",
        "                posterior_cov = torch.linalg.inv(prior_cov_inv + likelihood_cov_inv)\n",
        "                posterior_mean = posterior_cov @ (\n",
        "                    prior_cov_inv @ self.prior_mean[c] + likelihood_cov_inv @ sample_mean\n",
        "                )\n",
        "\n",
        "                self.prior_mean[c] = posterior_mean\n",
        "                self.prior_cov[c] = posterior_cov\n",
        "\n",
        "    def forward(self, features):\n",
        "        normalized_features = features / (torch.norm(features, dim=1, keepdim=True) + 1e-6)\n",
        "        normalized_prototypes = self.prior_mean / (torch.norm(self.prior_mean, dim=1, keepdim=True) + 1e-6)\n",
        "\n",
        "        cosine_similarity = torch.mm(normalized_features, normalized_prototypes.T)\n",
        "        return cosine_similarity\n",
        "\n",
        "def train_and_evaluate():\n",
        "    train_data_path = '/content/drive/MyDrive/ML/dataset/dataset/part_one_dataset/train_data'\n",
        "    eval_data_path = '/content/drive/MyDrive/ML/dataset/dataset/part_one_dataset/eval_data'\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    input_dim = 512\n",
        "    batch_size = 64\n",
        "    confidence_threshold = 0.75  # Confidence threshold for pseudo-labeling\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    feature_extractor = FeatureExtractor().to(device)\n",
        "\n",
        "    accuracy_matrix = np.zeros((10, 10))\n",
        "\n",
        "    for i in range(1, 11):\n",
        "        print(f\"\\n=== Processing Dataset D{i} ===\")\n",
        "\n",
        "        # Load the dataset\n",
        "        train_dataset = CustomCIFARDataset(\n",
        "            os.path.join(train_data_path, f'{i}_train_data.tar.pth'),\n",
        "            transform=transform,\n",
        "            labeled=(i == 1)  # Only D1 is labeled\n",
        "        )\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        features_list, labels_list = [], []\n",
        "        for batch in train_loader:\n",
        "            if i == 1:  # D1 is labeled\n",
        "                images, labels = batch\n",
        "                labels_list.append(labels)\n",
        "            else:  # D2 onward\n",
        "                images = batch\n",
        "\n",
        "            images = images.to(device)\n",
        "            features = feature_extractor(images)\n",
        "            features_list.append(features.cpu())\n",
        "\n",
        "        # Combine all features and labels\n",
        "        features = torch.cat(features_list).to(device)\n",
        "        if labels_list:  # Only for D1\n",
        "            labels = torch.cat(labels_list).to(device)\n",
        "            num_classes = len(torch.unique(labels))  # Dynamically determine num_classes\n",
        "        else:\n",
        "            labels = None\n",
        "            num_classes = 10  # Default for unlabeled datasets\n",
        "\n",
        "        # Initialize the classifier on D1\n",
        "        if i == 1:\n",
        "            classifier = BayesianClassifier(input_dim=input_dim, num_classes=num_classes).to(device)\n",
        "\n",
        "        if labels is not None:  # D1: Update posterior using true labels\n",
        "            classifier.update_posteriors(features, labels)\n",
        "        else:  # D2 onward: Pseudo-labeling and posterior update\n",
        "            with torch.no_grad():\n",
        "                logits = classifier(features)  # Predict using current means\n",
        "                predictions = logits.argmax(dim=1)  # Pseudo-labels\n",
        "                confidences = logits.max(dim=1).values  # Confidence scores\n",
        "\n",
        "            # Filter samples based on confidence threshold\n",
        "            high_confidence_indices = confidences > confidence_threshold\n",
        "            high_confidence_features = features[high_confidence_indices]\n",
        "            high_confidence_labels = predictions[high_confidence_indices]\n",
        "\n",
        "            # Update posterior using pseudo-labeled high-confidence samples\n",
        "            if len(high_confidence_features) > 0:\n",
        "                classifier.update_posteriors(high_confidence_features, high_confidence_labels)\n",
        "\n",
        "        print(f\"\\nEvaluating model f{i} on held-out datasets D̂1 to D̂{i}\")\n",
        "        for j in range(1, i + 1):\n",
        "            eval_dataset = CustomCIFARDataset(\n",
        "                os.path.join(eval_data_path, f'{j}_eval_data.tar.pth'),\n",
        "                transform=transform,\n",
        "                labeled=True\n",
        "            )\n",
        "            eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            correct, total = 0, 0\n",
        "            with torch.no_grad():\n",
        "                for images, labels in eval_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    features = feature_extractor(images)\n",
        "                    logits = classifier(features)\n",
        "                    predictions = logits.argmax(dim=1)\n",
        "                    correct += (predictions == labels).sum().item()\n",
        "                    total += labels.size(0)\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            accuracy_matrix[i - 1, j - 1] = accuracy\n",
        "            print(f\"Accuracy of model f{i} on D̂{j}: {accuracy:.2f}%\")\n",
        "\n",
        "    print(\"\\nAccuracy Matrix:\")\n",
        "    for row in accuracy_matrix:\n",
        "        print(\" \".join(f\"{val:6.2f}\" for val in row))\n",
        "\n",
        "# Run\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXsNpW84Qofp"
      },
      "source": [
        "using same model as above just with a efficient-net as feature extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSJlqzeNRgi5",
        "outputId": "813f1de9-bda4-49fa-ef4e-cd6b29c40086"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Processing Dataset D1 ===\n",
            "Fitting PCA on Dataset D1...\n",
            "\n",
            "Evaluating model f1 on held-out datasets D̂1 to D̂1\n",
            "Accuracy of model f1 on D̂1: 56.80%\n",
            "\n",
            "=== Processing Dataset D2 ===\n",
            "Transforming Dataset D2 with PCA...\n",
            "\n",
            "Evaluating model f2 on held-out datasets D̂1 to D̂2\n",
            "Accuracy of model f2 on D̂1: 59.04%\n",
            "Accuracy of model f2 on D̂2: 55.36%\n",
            "\n",
            "=== Processing Dataset D3 ===\n",
            "Transforming Dataset D3 with PCA...\n",
            "\n",
            "Evaluating model f3 on held-out datasets D̂1 to D̂3\n",
            "Accuracy of model f3 on D̂1: 57.56%\n",
            "Accuracy of model f3 on D̂2: 57.32%\n",
            "Accuracy of model f3 on D̂3: 56.32%\n",
            "\n",
            "=== Processing Dataset D4 ===\n",
            "Transforming Dataset D4 with PCA...\n",
            "\n",
            "Evaluating model f4 on held-out datasets D̂1 to D̂4\n",
            "Accuracy of model f4 on D̂1: 57.76%\n",
            "Accuracy of model f4 on D̂2: 56.24%\n",
            "Accuracy of model f4 on D̂3: 55.60%\n",
            "Accuracy of model f4 on D̂4: 56.68%\n",
            "\n",
            "=== Processing Dataset D5 ===\n",
            "Transforming Dataset D5 with PCA...\n",
            "\n",
            "Evaluating model f5 on held-out datasets D̂1 to D̂5\n",
            "Accuracy of model f5 on D̂1: 56.92%\n",
            "Accuracy of model f5 on D̂2: 56.96%\n",
            "Accuracy of model f5 on D̂3: 57.16%\n",
            "Accuracy of model f5 on D̂4: 56.92%\n",
            "Accuracy of model f5 on D̂5: 58.76%\n",
            "\n",
            "=== Processing Dataset D6 ===\n",
            "Transforming Dataset D6 with PCA...\n",
            "\n",
            "Evaluating model f6 on held-out datasets D̂1 to D̂6\n",
            "Accuracy of model f6 on D̂1: 57.52%\n",
            "Accuracy of model f6 on D̂2: 57.88%\n",
            "Accuracy of model f6 on D̂3: 56.28%\n",
            "Accuracy of model f6 on D̂4: 58.56%\n",
            "Accuracy of model f6 on D̂5: 57.64%\n",
            "Accuracy of model f6 on D̂6: 56.76%\n",
            "\n",
            "=== Processing Dataset D7 ===\n",
            "Transforming Dataset D7 with PCA...\n",
            "\n",
            "Evaluating model f7 on held-out datasets D̂1 to D̂7\n",
            "Accuracy of model f7 on D̂1: 58.04%\n",
            "Accuracy of model f7 on D̂2: 58.32%\n",
            "Accuracy of model f7 on D̂3: 56.36%\n",
            "Accuracy of model f7 on D̂4: 56.56%\n",
            "Accuracy of model f7 on D̂5: 57.48%\n",
            "Accuracy of model f7 on D̂6: 56.80%\n",
            "Accuracy of model f7 on D̂7: 56.08%\n",
            "\n",
            "=== Processing Dataset D8 ===\n",
            "Transforming Dataset D8 with PCA...\n",
            "\n",
            "Evaluating model f8 on held-out datasets D̂1 to D̂8\n",
            "Accuracy of model f8 on D̂1: 57.04%\n",
            "Accuracy of model f8 on D̂2: 58.04%\n",
            "Accuracy of model f8 on D̂3: 56.92%\n",
            "Accuracy of model f8 on D̂4: 58.48%\n",
            "Accuracy of model f8 on D̂5: 56.12%\n",
            "Accuracy of model f8 on D̂6: 56.96%\n",
            "Accuracy of model f8 on D̂7: 55.36%\n",
            "Accuracy of model f8 on D̂8: 55.52%\n",
            "\n",
            "=== Processing Dataset D9 ===\n",
            "Transforming Dataset D9 with PCA...\n",
            "\n",
            "Evaluating model f9 on held-out datasets D̂1 to D̂9\n",
            "Accuracy of model f9 on D̂1: 57.56%\n",
            "Accuracy of model f9 on D̂2: 56.36%\n",
            "Accuracy of model f9 on D̂3: 55.40%\n",
            "Accuracy of model f9 on D̂4: 57.52%\n",
            "Accuracy of model f9 on D̂5: 57.16%\n",
            "Accuracy of model f9 on D̂6: 57.36%\n",
            "Accuracy of model f9 on D̂7: 56.68%\n",
            "Accuracy of model f9 on D̂8: 57.44%\n",
            "Accuracy of model f9 on D̂9: 57.24%\n",
            "\n",
            "=== Processing Dataset D10 ===\n",
            "Transforming Dataset D10 with PCA...\n",
            "\n",
            "Evaluating model f10 on held-out datasets D̂1 to D̂10\n",
            "Accuracy of model f10 on D̂1: 58.24%\n",
            "Accuracy of model f10 on D̂2: 55.40%\n",
            "Accuracy of model f10 on D̂3: 55.64%\n",
            "Accuracy of model f10 on D̂4: 57.52%\n",
            "Accuracy of model f10 on D̂5: 57.56%\n",
            "Accuracy of model f10 on D̂6: 57.60%\n",
            "Accuracy of model f10 on D̂7: 55.32%\n",
            "Accuracy of model f10 on D̂8: 57.24%\n",
            "Accuracy of model f10 on D̂9: 56.88%\n",
            "Accuracy of model f10 on D̂10: 58.48%\n",
            "\n",
            "Accuracy Matrix:\n",
            " 56.80   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
            " 59.04  55.36   0.00   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
            " 57.56  57.32  56.32   0.00   0.00   0.00   0.00   0.00   0.00   0.00\n",
            " 57.76  56.24  55.60  56.68   0.00   0.00   0.00   0.00   0.00   0.00\n",
            " 56.92  56.96  57.16  56.92  58.76   0.00   0.00   0.00   0.00   0.00\n",
            " 57.52  57.88  56.28  58.56  57.64  56.76   0.00   0.00   0.00   0.00\n",
            " 58.04  58.32  56.36  56.56  57.48  56.80  56.08   0.00   0.00   0.00\n",
            " 57.04  58.04  56.92  58.48  56.12  56.96  55.36  55.52   0.00   0.00\n",
            " 57.56  56.36  55.40  57.52  57.16  57.36  56.68  57.44  57.24   0.00\n",
            " 58.24  55.40  55.64  57.52  57.56  57.60  55.32  57.24  56.88  58.48\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import efficientnet_b0\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Dataset Class\n",
        "class CustomCIFARDataset(Dataset):\n",
        "    def __init__(self, file_path, transform=None, labeled=True):\n",
        "        try:\n",
        "            data_dict = torch.load(file_path, weights_only=False)\n",
        "        except TypeError:\n",
        "            print(\"Warning: Falling back to weights_only=False. Ensure the file is trusted.\")\n",
        "            data_dict = torch.load(file_path)\n",
        "\n",
        "        self.images = data_dict['data']\n",
        "        self.labels = data_dict['targets'] if labeled else None\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        image = Image.fromarray(image)  # Convert to PIL Image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.labels is not None:\n",
        "            return image, self.labels[idx]\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "# Feature Extractor using EfficientNet\n",
        "class EfficientNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(EfficientNetFeatureExtractor, self).__init__()\n",
        "        base_model = efficientnet_b0(pretrained=pretrained)\n",
        "        self.features = nn.Sequential(*list(base_model.children())[:-2])  # Remove the classifier head\n",
        "\n",
        "        # Freeze the weights of the feature extractor\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)  # Extract features\n",
        "        return x.view(x.size(0), -1)  # Flatten the feature map\n",
        "\n",
        "class BayesianClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(BayesianClassifier, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.input_dim = input_dim\n",
        "        self.prior_mean = nn.Parameter(torch.zeros(num_classes, input_dim, dtype=torch.float32), requires_grad=False)  # Specify dtype as float32\n",
        "        self.prior_cov = nn.Parameter(torch.eye(input_dim, dtype=torch.float32).repeat(num_classes, 1, 1), requires_grad=False)  # Specify dtype as float32\n",
        "\n",
        "    def update_posteriors(self, features, labels):\n",
        "        # Ensure features is float32\n",
        "        features = features.type(torch.float32)\n",
        "        for c in range(self.num_classes):\n",
        "            class_features = features[labels == c]\n",
        "            if len(class_features) > 0:\n",
        "                n_c = len(class_features)\n",
        "                sample_mean = class_features.mean(dim=0)\n",
        "\n",
        "                # Ensure prior_cov_inv is float32\n",
        "                prior_cov_inv = torch.linalg.inv(self.prior_cov[c].type(torch.float32))\n",
        "                likelihood_cov_inv = torch.eye(self.input_dim, device=features.device, dtype=torch.float32) * n_c # Specify dtype as float32\n",
        "\n",
        "                posterior_cov = torch.linalg.inv(prior_cov_inv + likelihood_cov_inv)\n",
        "                posterior_mean = posterior_cov @ (\n",
        "                    prior_cov_inv @ self.prior_mean[c] + likelihood_cov_inv @ sample_mean\n",
        "                )\n",
        "\n",
        "                self.prior_mean.data[c] = posterior_mean\n",
        "                self.prior_cov.data[c] = posterior_cov\n",
        "\n",
        "    def forward(self, features):\n",
        "        # Convert features to float32\n",
        "        features = features.type(torch.float32)\n",
        "        normalized_features = features / (torch.norm(features, dim=1, keepdim=True) + 1e-6)\n",
        "        normalized_prototypes = self.prior_mean / (torch.norm(self.prior_mean, dim=1, keepdim=True) + 1e-6)\n",
        "\n",
        "        cosine_similarity = torch.mm(normalized_features, normalized_prototypes.T)\n",
        "        return cosine_similarity\n",
        "\n",
        "# Main Functionality\n",
        "def train_and_evaluate():\n",
        "    train_data_path = '/content/drive/MyDrive/ML/dataset/dataset/part_one_dataset/train_data'\n",
        "    eval_data_path = '/content/drive/MyDrive/ML/dataset/dataset/part_one_dataset/eval_data'\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.Resize((224, 224)),  # Resize for EfficientNet input\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize for EfficientNet\n",
        "    ])\n",
        "\n",
        "    batch_size = 64\n",
        "    reduced_dim = 128  # Target PCA dimensions\n",
        "    confidence_threshold = 0.8\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize Feature Extractor\n",
        "    feature_extractor = EfficientNetFeatureExtractor(pretrained=True).to(device)\n",
        "    pca = None  # PCA will be fitted on the first dataset (D1)\n",
        "\n",
        "    accuracy_matrix = np.zeros((10, 10))\n",
        "\n",
        "    for i in range(1, 11):\n",
        "        print(f\"\\n=== Processing Dataset D{i} ===\")\n",
        "\n",
        "        train_dataset = CustomCIFARDataset(\n",
        "            os.path.join(train_data_path, f'{i}_train_data.tar.pth'),\n",
        "            transform=transform,\n",
        "            labeled=(i == 1)\n",
        "        )\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        features_list, labels_list = [], []\n",
        "        for batch in train_loader:\n",
        "            if i == 1:  # D1 is labeled\n",
        "                images, labels = batch\n",
        "                labels_list.append(labels.to(device))\n",
        "            else:  # D2 onward\n",
        "                images = batch\n",
        "\n",
        "            images = images.to(device)\n",
        "            features = feature_extractor(images)  # Extract high-dimensional features\n",
        "            features_list.append(features)\n",
        "\n",
        "        features = torch.cat(features_list).to(device)  # Combine all features\n",
        "        if labels_list:\n",
        "            labels = torch.cat(labels_list).to(device)\n",
        "            num_classes = len(torch.unique(labels))\n",
        "        else:\n",
        "            labels = None\n",
        "            num_classes = 10  # Default for unlabeled datasets\n",
        "\n",
        "        # Apply PCA to reduce dimensions\n",
        "        if i == 1:\n",
        "            print(\"Fitting PCA on Dataset D1...\")\n",
        "            pca = PCA(n_components=reduced_dim)\n",
        "            features = torch.tensor(pca.fit_transform(features.cpu())).to(device)\n",
        "        else:\n",
        "            print(f\"Transforming Dataset D{i} with PCA...\")\n",
        "            features = torch.tensor(pca.transform(features.cpu())).to(device)\n",
        "\n",
        "        # Initialize Classifier for D1\n",
        "        if i == 1:\n",
        "            classifier = BayesianClassifier(input_dim=reduced_dim, num_classes=num_classes).to(device)\n",
        "\n",
        "        if labels is not None:  # D1: Update posterior using true labels\n",
        "            classifier.update_posteriors(features, labels)\n",
        "        else:  # D2 onward: Pseudo-labeling and posterior update\n",
        "            with torch.no_grad():\n",
        "                logits = classifier(features)  # Predict pseudo-labels\n",
        "                predictions = logits.argmax(dim=1)\n",
        "                confidences = logits.max(dim=1).values  # Confidence scores\n",
        "\n",
        "            # Filter samples based on confidence threshold\n",
        "            high_confidence_indices = confidences > confidence_threshold\n",
        "            high_confidence_features = features[high_confidence_indices]\n",
        "            high_confidence_labels = predictions[high_confidence_indices]\n",
        "\n",
        "            # Update posterior using high-confidence pseudo-labeled samples\n",
        "            if len(high_confidence_features) > 0:\n",
        "                classifier.update_posteriors(high_confidence_features, high_confidence_labels)\n",
        "\n",
        "        print(f\"\\nEvaluating model f{i} on held-out datasets D̂1 to D̂{i}\")\n",
        "        for j in range(1, i + 1):\n",
        "            eval_dataset = CustomCIFARDataset(\n",
        "                os.path.join(eval_data_path, f'{j}_eval_data.tar.pth'),\n",
        "                transform=transform,\n",
        "                labeled=True\n",
        "            )\n",
        "            eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            correct, total = 0, 0\n",
        "            with torch.no_grad():\n",
        "                for images, labels in eval_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    features = feature_extractor(images)\n",
        "                    features = torch.tensor(pca.transform(features.cpu())).to(device)  # Apply PCA\n",
        "                    logits = classifier(features)\n",
        "                    predictions = logits.argmax(dim=1)\n",
        "                    correct += (predictions == labels).sum().item()\n",
        "                    total += labels.size(0)\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            accuracy_matrix[i - 1, j - 1] = accuracy\n",
        "            print(f\"Accuracy of model f{i} on D̂{j}: {accuracy:.2f}%\")\n",
        "\n",
        "    print(\"\\nAccuracy Matrix:\")\n",
        "    for row in accuracy_matrix:\n",
        "        print(\" \".join(f\"{val:6.2f}\" for val in row))\n",
        "\n",
        "# Run\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using a kernalized PCA(gaussian PCA) for dimension reduction of the features obtained from the feature extractor"
      ],
      "metadata": {
        "id": "7hRHSoXdVdKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import efficientnet_b0\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.decomposition import KernelPCA  # Import KernelPCA for dimensionality reduction with a Gaussian kernel\n",
        "import matplotlib.pyplot as plt  # For visualizing the accuracy matrix\n",
        "\n",
        "# Custom dataset class for CIFAR-like data\n",
        "class CustomCIFARDataset(Dataset):\n",
        "    def __init__(self, file_path, transform=None, labeled=True):\n",
        "        # Load data from the specified file\n",
        "        try:\n",
        "            data_dict = torch.load(file_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading file {file_path}: {e}\")\n",
        "            raise e\n",
        "\n",
        "        self.images = data_dict['data']  # Image data\n",
        "        self.labels = data_dict['targets'] if labeled else None  # Labels if dataset is labeled\n",
        "        self.transform = transform  # Transformations to apply to images\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of samples in the dataset\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Retrieve an image and optionally its label\n",
        "        image = self.images[idx]\n",
        "        image = Image.fromarray(image)  # Convert numpy array to PIL Image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # Apply transformations\n",
        "\n",
        "        if self.labels is not None:\n",
        "            return image, self.labels[idx]  # Return image and label\n",
        "        else:\n",
        "            return image  # For unlabeled data, return only the image\n",
        "\n",
        "# Feature extractor based on EfficientNet\n",
        "class EfficientNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(EfficientNetFeatureExtractor, self).__init__()\n",
        "        # Load EfficientNet model and remove the classification head\n",
        "        base_model = efficientnet_b0(pretrained=pretrained)\n",
        "        self.features = nn.Sequential(*list(base_model.children())[:-1])\n",
        "\n",
        "        # Freeze the weights of the feature extractor\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Extract features and flatten the output\n",
        "        x = self.features(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        return x\n",
        "\n",
        "# Bayesian classifier for updating posteriors\n",
        "class BayesianClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(BayesianClassifier, self).__init__()\n",
        "        self.num_classes = num_classes  # Number of classes\n",
        "        self.input_dim = input_dim  # Dimensionality of the feature space\n",
        "\n",
        "        # Initialize prior means and covariances\n",
        "        self.prior_mean = nn.Parameter(torch.zeros(num_classes, input_dim, dtype=torch.float32), requires_grad=False)\n",
        "        self.prior_cov = nn.Parameter(torch.eye(input_dim, dtype=torch.float32).unsqueeze(0).repeat(num_classes, 1, 1), requires_grad=False)\n",
        "\n",
        "    def update_posteriors(self, features, labels):\n",
        "        # Update posterior mean and covariance for each class based on features\n",
        "        features = features.float()\n",
        "        for c in range(self.num_classes):\n",
        "            class_features = features[labels == c]  # Features of the current class\n",
        "            if len(class_features) > 0:\n",
        "                n_c = len(class_features)\n",
        "                sample_mean = class_features.mean(dim=0)\n",
        "\n",
        "                prior_cov_inv = torch.linalg.inv(self.prior_cov[c].float())\n",
        "                likelihood_cov_inv = torch.eye(self.input_dim, device=features.device, dtype=torch.float32) * n_c\n",
        "\n",
        "                posterior_cov = torch.linalg.inv(prior_cov_inv + likelihood_cov_inv)\n",
        "                posterior_mean = posterior_cov @ (prior_cov_inv @ self.prior_mean[c] + likelihood_cov_inv @ sample_mean)\n",
        "\n",
        "                # Update posterior mean and covariance\n",
        "                self.prior_mean.data[c] = posterior_mean\n",
        "                self.prior_cov.data[c] = posterior_cov\n",
        "\n",
        "    def forward(self, features):\n",
        "        # Compute class-wise cosine similarity for classification\n",
        "        features = features.float()\n",
        "        normalized_features = features / (torch.norm(features, dim=1, keepdim=True) + 1e-6)\n",
        "        normalized_prototypes = self.prior_mean / (torch.norm(self.prior_mean, dim=1, keepdim=True) + 1e-6)\n",
        "\n",
        "        # Cosine similarity between features and class prototypes\n",
        "        cosine_similarity = torch.mm(normalized_features, normalized_prototypes.T)\n",
        "        return cosine_similarity\n",
        "\n",
        "# Main training and evaluation function\n",
        "def train_and_evaluate():\n",
        "    # Paths for training and evaluation datasets\n",
        "    train_data_path = '/content/drive/MyDrive/CS771 A-2 /dataset/dataset/part_one_dataset/train_data'\n",
        "    eval_data_path = '/content/drive/MyDrive/CS771 A-2 /dataset/dataset/part_one_dataset/eval_data'\n",
        "\n",
        "    # Data transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.Resize((224, 224)),  # Resize for EfficientNet input size\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalization\n",
        "    ])\n",
        "\n",
        "    # Hyperparameters\n",
        "    batch_size = 64\n",
        "    reduced_dim = 128  # Dimensionality for Kernel PCA\n",
        "    confidence_threshold = 0.8\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize feature extractor\n",
        "    feature_extractor = EfficientNetFeatureExtractor(pretrained=True).to(device)\n",
        "    kernel_pca = None  # Kernel PCA object\n",
        "    accuracy_matrix = np.zeros((10, 10))  # Store accuracies across datasets\n",
        "\n",
        "    classifier = None  # Bayesian classifier object\n",
        "\n",
        "    for i in range(1, 11):\n",
        "        print(f\"\\n=== Processing Dataset D{i} ===\")\n",
        "\n",
        "        # Load training dataset\n",
        "        train_dataset = CustomCIFARDataset(\n",
        "            os.path.join(train_data_path, f'{i}_train_data.tar.pth'),\n",
        "            transform=transform,\n",
        "            labeled=(i == 1)  # Only Dataset D1 is labeled\n",
        "        )\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Extract features and labels\n",
        "        features_list, labels_list = [], []\n",
        "        for batch in train_loader:\n",
        "            if i == 1:  # D1 is labeled\n",
        "                images, labels = batch\n",
        "                labels_list.append(labels.to(device))\n",
        "            else:\n",
        "                images = batch\n",
        "\n",
        "            images = images.to(device)\n",
        "            features = feature_extractor(images)  # Extract features\n",
        "            features_list.append(features)\n",
        "\n",
        "        features = torch.cat(features_list).to(device)\n",
        "        if labels_list:\n",
        "            labels = torch.cat(labels_list).to(device)\n",
        "            num_classes = len(torch.unique(labels))\n",
        "        else:\n",
        "            labels = None\n",
        "            num_classes = 10\n",
        "\n",
        "        # Apply Gaussian Kernel PCA\n",
        "        if i == 1:\n",
        "            print(\"Fitting Gaussian Kernel PCA on Dataset D1...\")\n",
        "            kernel_pca = KernelPCA(n_components=reduced_dim, kernel='rbf', gamma=0.1)\n",
        "            features_np = features.cpu().numpy()\n",
        "            features_pca = kernel_pca.fit_transform(features_np)\n",
        "            features = torch.from_numpy(features_pca).float().to(device)\n",
        "        else:\n",
        "            print(f\"Transforming Dataset D{i} with Gaussian Kernel PCA...\")\n",
        "            features_np = features.cpu().numpy()\n",
        "            features_pca = kernel_pca.transform(features_np)\n",
        "            features = torch.from_numpy(features_pca).float().to(device)\n",
        "\n",
        "        # Initialize Bayesian classifier for D1\n",
        "        if i == 1:\n",
        "            classifier = BayesianClassifier(input_dim=reduced_dim, num_classes=num_classes).to(device)\n",
        "\n",
        "        if labels is not None:  # D1: Update posterior using true labels\n",
        "            classifier.update_posteriors(features, labels)\n",
        "        else:  # D2 onward: Pseudo-labeling\n",
        "            with torch.no_grad():\n",
        "                logits = classifier(features)\n",
        "                predictions = logits.argmax(dim=1)\n",
        "                confidences = logits.max(dim=1).values\n",
        "\n",
        "            high_conf_indices = confidences > confidence_threshold\n",
        "            high_conf_features = features[high_conf_indices]\n",
        "            high_conf_labels = predictions[high_conf_indices]\n",
        "\n",
        "            if len(high_conf_features) > 0:\n",
        "                classifier.update_posteriors(high_conf_features, high_conf_labels)\n",
        "\n",
        "        # Evaluate the model\n",
        "        print(f\"\\nEvaluating model f{i} on held-out datasets D̂1 to D̂{i}\")\n",
        "        for j in range(1, i + 1):\n",
        "            eval_dataset = CustomCIFARDataset(\n",
        "                os.path.join(eval_data_path, f'{j}_eval_data.tar.pth'),\n",
        "                transform=transform,\n",
        "                labeled=True\n",
        "            )\n",
        "            eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            correct, total = 0, 0\n",
        "            with torch.no_grad():\n",
        "                for eval_batch in eval_loader:\n",
        "                    eval_images, eval_labels = eval_batch\n",
        "                    eval_images, eval_labels = eval_images.to(device), eval_labels.to(device)\n",
        "\n",
        "                    eval_features = feature_extractor(eval_images)\n",
        "                    eval_features_np = eval_features.cpu().numpy()\n",
        "                    eval_features_pca = kernel_pca.transform(eval_features_np)\n",
        "                    eval_features_pca = torch.from_numpy(eval_features_pca).float().to(device)\n",
        "                    logits = classifier(eval_features_pca)\n",
        "                    predictions = logits.argmax(dim=1)\n",
        "                    correct += (predictions == eval_labels).sum().item()\n",
        "                    total += eval_labels.size(0)\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            accuracy_matrix[i - 1][j - 1] = accuracy\n",
        "            print(f\"Accuracy on D̂{j}: {accuracy:.2f}%\")\n",
        "\n",
        "    print(\"\\nFinal Accuracy Matrix:\")\n",
        "    print(accuracy_matrix)\n",
        "\n",
        "    plt.imshow(accuracy_matrix, cmap='viridis', interpolation='nearest')\n",
        "    plt.colorbar()\n",
        "    plt.title(\"Accuracy Matrix\")\n",
        "    plt.xlabel(\"Datasets D̂j\")\n",
        "    plt.ylabel(\"Datasets Di\")\n",
        "    plt.show()\n",
        "\n",
        "# Entry point\n",
        "if __name__ == '__main__':\n",
        "    train_and_evaluate()\n"
      ],
      "metadata": {
        "id": "ZRJJUBF3VnDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using polynimical kernel PCA"
      ],
      "metadata": {
        "id": "KdzujCejVyH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import efficientnet_b0\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Custom dataset class to handle CIFAR-like dataset loading and transformations\n",
        "class CustomCIFARDataset(Dataset):\n",
        "    def __init__(self, file_path, transform=None, labeled=True):\n",
        "        \"\"\"\n",
        "        Initialize the dataset with file path and transformations.\n",
        "        :param file_path: Path to the dataset file.\n",
        "        :param transform: Transformations to be applied on the images.\n",
        "        :param labeled: Indicates if the dataset contains labels.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            data_dict = torch.load(file_path, weights_only=False)\n",
        "        except TypeError:\n",
        "            print(\"Warning: Falling back to weights_only=False. Ensure the file is trusted.\")\n",
        "            data_dict = torch.load(file_path)\n",
        "\n",
        "        # Load images and optionally labels\n",
        "        self.images = data_dict['data']\n",
        "        self.labels = data_dict['targets'] if labeled else None\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the total number of images in the dataset.\"\"\"\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve an image (and its label if available) by index.\n",
        "        :param idx: Index of the desired sample.\n",
        "        :return: Transformed image and label (if available).\n",
        "        \"\"\"\n",
        "        image = self.images[idx]\n",
        "        image = Image.fromarray(image)  # Convert numpy array to PIL Image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.labels is not None:\n",
        "            return image, self.labels[idx]\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "# Feature extractor using pre-trained EfficientNet model\n",
        "class EfficientNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        \"\"\"\n",
        "        Initialize EfficientNet-based feature extractor.\n",
        "        :param pretrained: Use pre-trained weights if True.\n",
        "        \"\"\"\n",
        "        super(EfficientNetFeatureExtractor, self).__init__()\n",
        "        base_model = efficientnet_b0(pretrained=pretrained)\n",
        "        # Extract all layers except the classification head\n",
        "        self.features = nn.Sequential(*list(base_model.children())[:-2])\n",
        "\n",
        "        # Freeze the feature extractor layers to avoid training them\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the feature extractor.\n",
        "        :param x: Input images.\n",
        "        :return: Flattened feature maps.\n",
        "        \"\"\"\n",
        "        x = self.features(x)  # Extract features\n",
        "        return x.view(x.size(0), -1)  # Flatten the feature map\n",
        "\n",
        "# Bayesian Classifier with prior updates for pseudo-labeling\n",
        "class BayesianClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        \"\"\"\n",
        "        Initialize the Bayesian Classifier.\n",
        "        :param input_dim: Dimensionality of the input features.\n",
        "        :param num_classes: Number of classes for classification.\n",
        "        \"\"\"\n",
        "        super(BayesianClassifier, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.input_dim = input_dim\n",
        "        # Initialize prior mean and covariance for each class\n",
        "        self.prior_mean = nn.Parameter(torch.zeros(num_classes, input_dim, dtype=torch.float32), requires_grad=False)\n",
        "        self.prior_cov = nn.Parameter(torch.eye(input_dim, dtype=torch.float32).repeat(num_classes, 1, 1), requires_grad=False)\n",
        "\n",
        "    def update_posteriors(self, features, labels):\n",
        "        \"\"\"\n",
        "        Update the posterior mean and covariance based on new samples.\n",
        "        :param features: Feature vectors of the samples.\n",
        "        :param labels: Labels corresponding to the samples.\n",
        "        \"\"\"\n",
        "        features = features.type(torch.float32)  # Ensure correct dtype\n",
        "        for c in range(self.num_classes):\n",
        "            class_features = features[labels == c]\n",
        "            if len(class_features) > 0:\n",
        "                n_c = len(class_features)\n",
        "                sample_mean = class_features.mean(dim=0)\n",
        "\n",
        "                prior_cov_inv = torch.linalg.inv(self.prior_cov[c].type(torch.float32))\n",
        "                likelihood_cov_inv = torch.eye(self.input_dim, device=features.device, dtype=torch.float32) * n_c\n",
        "\n",
        "                posterior_cov = torch.linalg.inv(prior_cov_inv + likelihood_cov_inv)\n",
        "                posterior_mean = posterior_cov @ (\n",
        "                    prior_cov_inv @ self.prior_mean[c] + likelihood_cov_inv @ sample_mean\n",
        "                )\n",
        "\n",
        "                self.prior_mean.data[c] = posterior_mean\n",
        "                self.prior_cov.data[c] = posterior_cov\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        Perform classification using cosine similarity.\n",
        "        :param features: Input features for classification.\n",
        "        :return: Cosine similarity scores with the class prototypes.\n",
        "        \"\"\"\n",
        "        features = features.type(torch.float32)  # Ensure correct dtype\n",
        "        normalized_features = features / (torch.norm(features, dim=1, keepdim=True) + 1e-6)\n",
        "        normalized_prototypes = self.prior_mean / (torch.norm(self.prior_mean, dim=1, keepdim=True) + 1e-6)\n",
        "        cosine_similarity = torch.mm(normalized_features, normalized_prototypes.T)\n",
        "        return cosine_similarity\n",
        "\n",
        "# Main training and evaluation routine\n",
        "def train_and_evaluate():\n",
        "    \"\"\"\n",
        "    Train and evaluate the Bayesian classifier on a sequence of datasets\n",
        "    with feature extraction, dimensionality reduction, and pseudo-labeling.\n",
        "    \"\"\"\n",
        "    # Dataset paths\n",
        "    train_data_path = '/content/drive/MyDrive/CS771 A-2 /dataset/dataset/part_one_dataset/train_data'\n",
        "    eval_data_path = '/content/drive/MyDrive/CS771 A-2 /dataset/dataset/part_one_dataset/eval_data'\n",
        "\n",
        "    # Define data transformations for image preprocessing\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.Resize((224, 224)),  # Resize for EfficientNet input\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # EfficientNet normalization\n",
        "    ])\n",
        "\n",
        "    # Configuration parameters\n",
        "    batch_size = 64\n",
        "    reduced_dim = 128  # Dimensionality after Kernel PCA\n",
        "    confidence_threshold = 0.8  # Minimum confidence for pseudo-labeling\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize feature extractor and variables\n",
        "    feature_extractor = EfficientNetFeatureExtractor(pretrained=True).to(device)\n",
        "    pca = None  # Kernel PCA object\n",
        "    accuracy_matrix = np.zeros((10, 10))  # Accuracy storage matrix\n",
        "\n",
        "    for i in range(1, 11):\n",
        "        print(f\"\\n=== Processing Dataset D{i} ===\")\n",
        "\n",
        "        # Load training dataset\n",
        "        train_dataset = CustomCIFARDataset(\n",
        "            os.path.join(train_data_path, f'{i}_train_data.tar.pth'),\n",
        "            transform=transform,\n",
        "            labeled=(i == 1)  # Labels only for D1\n",
        "        )\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        features_list, labels_list = [], []\n",
        "        for batch in train_loader:\n",
        "            if i == 1:\n",
        "                images, labels = batch\n",
        "                labels_list.append(labels.to(device))\n",
        "            else:\n",
        "                images = batch\n",
        "\n",
        "            images = images.to(device)\n",
        "            features = feature_extractor(images)\n",
        "            features_list.append(features)\n",
        "\n",
        "        features = torch.cat(features_list).to(device)\n",
        "        if labels_list:\n",
        "            labels = torch.cat(labels_list).to(device)\n",
        "            num_classes = len(torch.unique(labels))\n",
        "        else:\n",
        "            labels = None\n",
        "            num_classes = 10\n",
        "\n",
        "        if i == 1:\n",
        "            print(\"Fitting Polynomial Kernel PCA on Dataset D1...\")\n",
        "            pca = KernelPCA(n_components=reduced_dim, kernel='poly', degree=3, coef0=1)\n",
        "            features = torch.tensor(pca.fit_transform(features.cpu())).to(device)\n",
        "            classifier = BayesianClassifier(input_dim=reduced_dim, num_classes=num_classes).to(device)\n",
        "        else:\n",
        "            print(f\"Transforming Dataset D{i} with Polynomial Kernel PCA...\")\n",
        "            features = torch.tensor(pca.transform(features.cpu())).to(device)\n",
        "\n",
        "        if labels is not None:\n",
        "            classifier.update_posteriors(features, labels)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                logits = classifier(features)\n",
        "                predictions = logits.argmax(dim=1)\n",
        "                confidences = logits.max(dim=1).values\n",
        "\n",
        "            high_conf_indices = confidences > confidence_threshold\n",
        "            high_conf_features = features[high_conf_indices]\n",
        "            high_conf_labels = predictions[high_conf_indices]\n",
        "\n",
        "            if len(high_conf_features) > 0:\n",
        "                classifier.update_posteriors(high_conf_features, high_conf_labels)\n",
        "\n",
        "        print(f\"\\nEvaluating model f{i} on held-out datasets D̂1 to D̂{i}\")\n",
        "        for j in range(1, i + 1):\n",
        "            eval_dataset = CustomCIFARDataset(\n",
        "                os.path.join(eval_data_path, f'{j}_eval_data.tar.pth'),\n",
        "                transform=transform,\n",
        "                labeled=True\n",
        "            )\n",
        "            eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            correct, total = 0, 0\n",
        "            with torch.no_grad():\n",
        "                for images, labels in eval_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    features = feature_extractor(images)\n",
        "                    features = torch.tensor(pca.transform(features.cpu())).to(device)\n",
        "                    logits = classifier(features)\n",
        "                    predictions = logits.argmax(dim=1)\n",
        "                    correct += (predictions == labels).sum().item()\n",
        "                    total += labels.size(0)\n",
        "\n",
        "            accuracy = correct / total\n",
        "            accuracy_matrix[i - 1][j - 1] = accuracy\n",
        "            print(f\"f{i} accuracy on D̂{j}: {accuracy:.4f}\")\n",
        "\n",
        "    np.save('accuracy_matrix.npy', accuracy_matrix)\n",
        "    print(\"\\nTraining and evaluation completed.\")\n"
      ],
      "metadata": {
        "id": "hEI6oTmhWCEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using RBF kernel PCA"
      ],
      "metadata": {
        "id": "kc31IyTXWC7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import efficientnet_b0\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.decomposition import KernelPCA  # For dimensionality reduction using RBF Kernel PCA\n",
        "\n",
        "# Custom Dataset Class for CIFAR-like data\n",
        "class CustomCIFARDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset class to handle CIFAR-like data.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): Path to the dataset file.\n",
        "        transform (callable, optional): Transformations to apply to images.\n",
        "        labeled (bool): Whether the dataset has labels (for training).\n",
        "    \"\"\"\n",
        "    def __init__(self, file_path, transform=None, labeled=True):\n",
        "        try:\n",
        "            # Load the dataset file\n",
        "            data_dict = torch.load(file_path, weights_only=False)\n",
        "        except TypeError:\n",
        "            print(\"Warning: Falling back to weights_only=False. Ensure the file is trusted.\")\n",
        "            data_dict = torch.load(file_path)\n",
        "\n",
        "        self.images = data_dict['data']  # Image data\n",
        "        self.labels = data_dict['targets'] if labeled else None  # Labels, if available\n",
        "        self.transform = transform  # Transformations to apply to the images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)  # Total number of images\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieve an image and its label (if available) by index.\n",
        "        \"\"\"\n",
        "        image = self.images[idx]\n",
        "        image = Image.fromarray(image)  # Convert NumPy array to PIL Image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # Apply transformations\n",
        "\n",
        "        if self.labels is not None:\n",
        "            return image, self.labels[idx]  # Return image and label\n",
        "        else:\n",
        "            return image  # Return only the image for unlabeled datasets\n",
        "\n",
        "# EfficientNet-based Feature Extractor\n",
        "class EfficientNetFeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Feature extractor using EfficientNet B0. Pretrained weights are used for\n",
        "    transfer learning, and the classification head is removed.\n",
        "    \"\"\"\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(EfficientNetFeatureExtractor, self).__init__()\n",
        "        base_model = efficientnet_b0(pretrained=pretrained)\n",
        "        self.features = nn.Sequential(*list(base_model.children())[:-2])  # Remove the classifier head\n",
        "\n",
        "        # Freeze all parameters to prevent training\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass to extract features from input images.\n",
        "        \"\"\"\n",
        "        x = self.features(x)  # Extract feature maps\n",
        "        return x.view(x.size(0), -1)  # Flatten the feature maps into 1D vectors\n",
        "\n",
        "# Bayesian Classifier with Posterior Updates\n",
        "class BayesianClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    Bayesian Classifier to predict class probabilities using learned posteriors.\n",
        "\n",
        "    Args:\n",
        "        input_dim (int): Dimensionality of input features.\n",
        "        num_classes (int): Number of target classes.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(BayesianClassifier, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "        # Initialize prior mean and covariance for each class\n",
        "        self.prior_mean = nn.Parameter(torch.zeros(num_classes, input_dim, dtype=torch.float32), requires_grad=False)\n",
        "        self.prior_cov = nn.Parameter(torch.eye(input_dim, dtype=torch.float32).repeat(num_classes, 1, 1), requires_grad=False)\n",
        "\n",
        "    def update_posteriors(self, features, labels):\n",
        "        \"\"\"\n",
        "        Update the posterior distribution for each class using labeled data.\n",
        "        \"\"\"\n",
        "        features = features.type(torch.float32)  # Ensure features are float32 for computation\n",
        "        for c in range(self.num_classes):\n",
        "            class_features = features[labels == c]  # Extract features of class `c`\n",
        "            if len(class_features) > 0:\n",
        "                n_c = len(class_features)  # Number of samples in the class\n",
        "                sample_mean = class_features.mean(dim=0)  # Compute mean of the class features\n",
        "\n",
        "                # Compute posterior covariance and mean using Bayes' rule\n",
        "                prior_cov_inv = torch.linalg.inv(self.prior_cov[c].type(torch.float32))\n",
        "                likelihood_cov_inv = torch.eye(self.input_dim, dtype=torch.float32, device=features.device) * n_c\n",
        "                posterior_cov = torch.linalg.inv(prior_cov_inv + likelihood_cov_inv)\n",
        "                posterior_mean = posterior_cov @ (\n",
        "                    prior_cov_inv @ self.prior_mean[c] + likelihood_cov_inv @ sample_mean\n",
        "                )\n",
        "\n",
        "                # Update the prior with the computed posterior\n",
        "                self.prior_mean.data[c] = posterior_mean\n",
        "                self.prior_cov.data[c] = posterior_cov\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        Forward pass to compute similarity scores between features and class prototypes.\n",
        "        \"\"\"\n",
        "        features = features.type(torch.float32)\n",
        "        normalized_features = features / (torch.norm(features, dim=1, keepdim=True) + 1e-6)  # Normalize input features\n",
        "        normalized_prototypes = self.prior_mean / (torch.norm(self.prior_mean, dim=1, keepdim=True) + 1e-6)  # Normalize prototypes\n",
        "\n",
        "        cosine_similarity = torch.mm(normalized_features, normalized_prototypes.T)  # Compute cosine similarity\n",
        "        return cosine_similarity\n",
        "\n",
        "# Training and Evaluation Workflow\n",
        "def train_and_evaluate():\n",
        "    \"\"\"\n",
        "    Main training and evaluation pipeline. This includes:\n",
        "    - Feature extraction with EfficientNet.\n",
        "    - Dimensionality reduction using RBF Kernel PCA.\n",
        "    - Training Bayesian Classifier on labeled and pseudo-labeled data.\n",
        "    - Evaluating models on test datasets.\n",
        "    \"\"\"\n",
        "    train_data_path = '/path/to/train_data'\n",
        "    eval_data_path = '/path/to/eval_data'\n",
        "\n",
        "    # Image transformations for data augmentation and normalization\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.Resize((224, 224)),  # EfficientNet input size\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # EfficientNet normalization\n",
        "    ])\n",
        "\n",
        "    batch_size = 64\n",
        "    reduced_dim = 128  # Target dimensionality after RBF Kernel PCA\n",
        "    confidence_threshold = 0.8\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize feature extractor\n",
        "    feature_extractor = EfficientNetFeatureExtractor(pretrained=True).to(device)\n",
        "    rbf_kernel_pca = None  # RBF Kernel PCA instance\n",
        "\n",
        "    accuracy_matrix = np.zeros((10, 10))  # Matrix to store accuracy results\n",
        "\n",
        "    for i in range(1, 11):  # Iterate over datasets D1 to D10\n",
        "        print(f\"\\n=== Processing Dataset D{i} ===\")\n",
        "\n",
        "        # Load training data\n",
        "        train_dataset = CustomCIFARDataset(\n",
        "            os.path.join(train_data_path, f'{i}_train_data.tar.pth'),\n",
        "            transform=transform,\n",
        "            labeled=(i == 1)  # Only D1 is labeled\n",
        "        )\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        # Extract features for training data\n",
        "        features_list, labels_list = [], []\n",
        "        for batch in train_loader:\n",
        "            if i == 1:\n",
        "                images, labels = batch\n",
        "                labels_list.append(labels.to(device))\n",
        "            else:\n",
        "                images = batch\n",
        "\n",
        "            images = images.to(device)\n",
        "            features = feature_extractor(images)\n",
        "            features_list.append(features)\n",
        "\n",
        "        features = torch.cat(features_list).to(device)\n",
        "        if labels_list:\n",
        "            labels = torch.cat(labels_list).to(device)\n",
        "            num_classes = len(torch.unique(labels))\n",
        "        else:\n",
        "            labels = None\n",
        "            num_classes = 10  # Default number of classes for unlabeled data\n",
        "\n",
        "        # Apply RBF Kernel PCA\n",
        "        if i == 1:\n",
        "            print(\"Fitting RBF Kernel PCA on Dataset D1...\")\n",
        "            rbf_kernel_pca = KernelPCA(n_components=reduced_dim, kernel=\"rbf\", gamma=0.1)\n",
        "            features = torch.tensor(rbf_kernel_pca.fit_transform(features.cpu())).to(device)\n",
        "        else:\n",
        "            print(f\"Transforming Dataset D{i} with RBF Kernel PCA...\")\n",
        "            features = torch.tensor(rbf_kernel_pca.transform(features.cpu())).to(device)\n",
        "\n",
        "        # Initialize the classifier for D1\n",
        "        if i == 1:\n",
        "            classifier = BayesianClassifier(input_dim=reduced_dim, num_classes=num_classes).to(device)\n",
        "\n",
        "        # Update classifier posteriors\n",
        "        if labels is not None:  # For D1\n",
        "            classifier.update_posteriors(features, labels)\n",
        "        else:  # Pseudo-labeling for D2 onward\n",
        "            with torch.no_grad():\n",
        "                logits = classifier(features)\n",
        "                predictions = logits.argmax(dim=1)\n",
        "                confidences = logits.max(dim=1).values\n",
        "\n",
        "            high_confidence_indices = confidences > confidence_threshold\n",
        "            high_confidence_features = features[high_confidence_indices]\n",
        "            high_confidence_labels = predictions[high_confidence_indices]\n",
        "\n",
        "            if len(high_confidence_features) > 0:\n",
        "                classifier.update_posteriors(high_confidence_features, high_confidence_labels)\n",
        "\n",
        "        # Evaluate the model\n",
        "        print(f\"\\nEvaluating model f{i} on held-out datasets D̂1 to D̂{i}\")\n",
        "        for j in range(1, i + 1):\n",
        "            eval_dataset = CustomCIFARDataset(\n",
        "                os.path.join(eval_data_path, f'{j}_eval_data.tar.pth'),\n",
        "                transform=transform,\n",
        "                labeled=False  # Test data is unlabeled\n",
        "            )\n",
        "            eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            predictions = []\n",
        "            for images in eval_loader:\n",
        "                images = images.to(device)\n",
        "                features = feature_extractor(images)\n",
        "                features = torch.tensor(rbf_kernel_pca.transform(features.cpu())).to(device)\n",
        "                logits = classifier(features)\n",
        "                predictions.extend(logits.argmax(dim=1).cpu().numpy())\n",
        "\n",
        "            eval_predictions = np.array(predictions)\n",
        "            accuracy_matrix[i - 1][j - 1] = 0  # Replace with actual accuracy computation\n",
        "            print(f\"Accuracy of f{i} on D̂{j}: {accuracy_matrix[i - 1][j - 1]:.3f}\")\n",
        "\n",
        "    return accuracy_matrix\n"
      ],
      "metadata": {
        "id": "cPJILjN8WJTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Cosine similarity PCA"
      ],
      "metadata": {
        "id": "uYNjPyvRWNVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from torchvision.models import efficientnet_b0\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "# Dataset Class\n",
        "class CustomCIFARDataset(Dataset):\n",
        "    def __init__(self, file_path, transform=None, labeled=True):\n",
        "        \"\"\"\n",
        "        Custom Dataset class to load CIFAR-like data.\n",
        "\n",
        "        :param file_path: Path to the dataset file.\n",
        "        :param transform: Transformations to apply to images.\n",
        "        :param labeled: Boolean indicating if the dataset contains labels.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            data_dict = torch.load(file_path, weights_only=False)\n",
        "        except TypeError:\n",
        "            print(\"Warning: Falling back to weights_only=False. Ensure the file is trusted.\")\n",
        "            data_dict = torch.load(file_path)\n",
        "\n",
        "        # Load the images and labels from the dataset\n",
        "        self.images = data_dict['data']\n",
        "        self.labels = data_dict['targets'] if labeled else None\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of images in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Retrieves the image and its label (if available) at the given index.\n",
        "\n",
        "        :param idx: Index of the sample to retrieve.\n",
        "        :return: Transformed image and its corresponding label (if available).\n",
        "        \"\"\"\n",
        "        image = self.images[idx]\n",
        "        image = Image.fromarray(image)  # Convert to PIL Image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)  # Apply any transformations\n",
        "\n",
        "        if self.labels is not None:\n",
        "            return image, self.labels[idx]\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "# Feature Extractor using EfficientNet\n",
        "class EfficientNetFeatureExtractor(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        \"\"\"\n",
        "        Initializes the EfficientNet feature extractor by removing the classifier head.\n",
        "\n",
        "        :param pretrained: Boolean indicating whether to load pretrained weights.\n",
        "        \"\"\"\n",
        "        super(EfficientNetFeatureExtractor, self).__init__()\n",
        "        base_model = efficientnet_b0(pretrained=pretrained)\n",
        "        # Remove the classifier head by selecting all layers except the last two\n",
        "        self.features = nn.Sequential(*list(base_model.children())[:-2])\n",
        "\n",
        "        # Freeze the weights of the feature extractor (no backpropagation)\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass to extract features from the input image.\n",
        "\n",
        "        :param x: Input image tensor.\n",
        "        :return: Flattened feature tensor.\n",
        "        \"\"\"\n",
        "        x = self.features(x)  # Extract features\n",
        "        return x.view(x.size(0), -1)  # Flatten the feature map to a vector\n",
        "\n",
        "# Bayesian Classifier using Cosine Similarity\n",
        "class BayesianClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        \"\"\"\n",
        "        Initializes the Bayesian Classifier with prior mean and covariance.\n",
        "\n",
        "        :param input_dim: Dimensionality of the feature space.\n",
        "        :param num_classes: Number of output classes.\n",
        "        \"\"\"\n",
        "        super(BayesianClassifier, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.input_dim = input_dim\n",
        "        # Initialize prior mean and covariance matrices as non-trainable parameters\n",
        "        self.prior_mean = nn.Parameter(torch.zeros(num_classes, input_dim, dtype=torch.float32), requires_grad=False)\n",
        "        self.prior_cov = nn.Parameter(torch.eye(input_dim, dtype=torch.float32).repeat(num_classes, 1, 1), requires_grad=False)\n",
        "\n",
        "    def update_posteriors(self, features, labels):\n",
        "        \"\"\"\n",
        "        Updates the posterior mean and covariance for each class based on the input features and labels.\n",
        "\n",
        "        :param features: The feature vectors of the training samples.\n",
        "        :param labels: The true labels of the samples.\n",
        "        \"\"\"\n",
        "        features = features.type(torch.float32)  # Ensure features are in float32\n",
        "        for c in range(self.num_classes):\n",
        "            class_features = features[labels == c]  # Select features corresponding to class c\n",
        "            if len(class_features) > 0:\n",
        "                n_c = len(class_features)\n",
        "                sample_mean = class_features.mean(dim=0)\n",
        "\n",
        "                # Inverse of prior covariance matrix for class c\n",
        "                prior_cov_inv = torch.linalg.inv(self.prior_cov[c].type(torch.float32))\n",
        "                likelihood_cov_inv = torch.eye(self.input_dim, device=features.device, dtype=torch.float32) * n_c\n",
        "\n",
        "                posterior_cov = torch.linalg.inv(prior_cov_inv + likelihood_cov_inv)\n",
        "                posterior_mean = posterior_cov @ (\n",
        "                    prior_cov_inv @ self.prior_mean[c] + likelihood_cov_inv @ sample_mean\n",
        "                )\n",
        "\n",
        "                # Update the class's prior mean and covariance with posterior values\n",
        "                self.prior_mean.data[c] = posterior_mean\n",
        "                self.prior_cov.data[c] = posterior_cov\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"\n",
        "        Performs a forward pass to compute the cosine similarity between features and class prototypes.\n",
        "\n",
        "        :param features: Input feature tensor to classify.\n",
        "        :return: Cosine similarity between input features and class prototypes.\n",
        "        \"\"\"\n",
        "        features = features.type(torch.float32)  # Ensure features are in float32\n",
        "        normalized_features = features / (torch.norm(features, dim=1, keepdim=True) + 1e-6)\n",
        "        normalized_prototypes = self.prior_mean / (torch.norm(self.prior_mean, dim=1, keepdim=True) + 1e-6)\n",
        "\n",
        "        # Compute cosine similarity between features and class prototypes\n",
        "        cosine_similarity = torch.mm(normalized_features, normalized_prototypes.T)\n",
        "        return cosine_similarity\n",
        "\n",
        "# Main Functionality\n",
        "def train_and_evaluate():\n",
        "    \"\"\"\n",
        "    Main function to train and evaluate the model using KernelPCA and Bayesian Classification.\n",
        "    \"\"\"\n",
        "    train_data_path = '/content/drive/MyDrive/CS771 A-2 /dataset/dataset/part_one_dataset/train_data'\n",
        "    eval_data_path = '/content/drive/MyDrive/CS771 A-2 /dataset/dataset/part_one_dataset/eval_data'\n",
        "\n",
        "    # Define transformations to apply on the images\n",
        "    transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.Resize((224, 224)),  # Resize for EfficientNet input\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])  # Normalize for EfficientNet\n",
        "    ])\n",
        "\n",
        "    batch_size = 64\n",
        "    reduced_dim = 128  # Target KernelPCA dimensions\n",
        "    confidence_threshold = 0.8  # Confidence threshold for pseudo-labeling\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize Feature Extractor and KernelPCA (empty at first)\n",
        "    feature_extractor = EfficientNetFeatureExtractor(pretrained=True).to(device)\n",
        "    kernel_pca = None  # KernelPCA will be fitted on the first dataset (D1)\n",
        "\n",
        "    accuracy_matrix = np.zeros((10, 10))\n",
        "\n",
        "    # Process each dataset from D1 to D10\n",
        "    for i in range(1, 11):\n",
        "        print(f\"\\n=== Processing Dataset D{i} ===\")\n",
        "\n",
        "        # Load training data for the current dataset\n",
        "        train_dataset = CustomCIFARDataset(\n",
        "            os.path.join(train_data_path, f'{i}_train_data.tar.pth'),\n",
        "            transform=transform,\n",
        "            labeled=(i == 1)  # Only D1 is labeled\n",
        "        )\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        features_list, labels_list = [], []\n",
        "        for batch in train_loader:\n",
        "            if i == 1:  # D1 is labeled\n",
        "                images, labels = batch\n",
        "                labels_list.append(labels.to(device))\n",
        "            else:  # D2 onward is unlabeled\n",
        "                images = batch\n",
        "\n",
        "            images = images.to(device)\n",
        "            features = feature_extractor(images)  # Extract high-dimensional features\n",
        "            features_list.append(features)\n",
        "\n",
        "        features = torch.cat(features_list).to(device)  # Combine all extracted features\n",
        "        if labels_list:\n",
        "            labels = torch.cat(labels_list).to(device)\n",
        "            num_classes = len(torch.unique(labels))\n",
        "        else:\n",
        "            labels = None\n",
        "            num_classes = 10  # Default for unlabeled datasets\n",
        "\n",
        "        # Apply Kernel PCA to reduce dimensions\n",
        "        if i == 1:\n",
        "            print(\"Fitting KernelPCA on Dataset D1...\")\n",
        "            kernel_pca = KernelPCA(n_components=reduced_dim, kernel=\"cosine\")\n",
        "            features = torch.tensor(kernel_pca.fit_transform(features.cpu())).to(device)  # Fit on D1\n",
        "        else:\n",
        "            print(f\"Transforming Dataset D{i} with KernelPCA...\")\n",
        "            features = torch.tensor(kernel_pca.transform(features.cpu())).to(device)  # Apply to other datasets\n",
        "\n",
        "        # Initialize Bayesian Classifier for D1\n",
        "        if i == 1:\n",
        "            classifier = BayesianClassifier(input_dim=reduced_dim, num_classes=num_classes).to(device)\n",
        "\n",
        "        # Update posteriors for D1 with true labels\n",
        "        if labels is not None:  # D1\n",
        "            classifier.update_posteriors(features, labels)\n",
        "        else:  # D2 onward: Pseudo-labeling and posterior update\n",
        "            with torch.no_grad():\n",
        "                logits = classifier(features)  # Predict pseudo-labels\n",
        "                predictions = logits.argmax(dim=1)\n",
        "                confidences = logits.max(dim=1).values  # Confidence scores\n",
        "\n",
        "            # Filter samples based on confidence threshold\n",
        "            high_confidence_indices = confidences > confidence_threshold\n",
        "            high_confidence_features = features[high_confidence_indices]\n",
        "            high_confidence_predictions = predictions[high_confidence_indices]\n",
        "\n",
        "            # Update classifier with high-confidence predictions\n",
        "            classifier.update_posteriors(high_confidence_features, high_confidence_predictions)\n",
        "\n",
        "        # Evaluate using the classifier for test set D10\n",
        "        if i == 10:\n",
        "            eval_dataset = CustomCIFARDataset(\n",
        "                os.path.join(eval_data_path, 'eval_data.tar.pth'),\n",
        "                transform=transform,\n",
        "                labeled=True  # Evaluation dataset is labeled\n",
        "            )\n",
        "            eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            all_preds, all_labels = [], []\n",
        "            for batch in eval_loader:\n",
        "                images, labels = batch\n",
        "                images = images.to(device)\n",
        "                features = feature_extractor(images)\n",
        "                features = torch.tensor(kernel_pca.transform(features.cpu())).to(device)  # Apply KernelPCA\n",
        "                logits = classifier(features)\n",
        "                predictions = logits.argmax(dim=1)\n",
        "\n",
        "                all_preds.append(predictions.cpu().numpy())\n",
        "                all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "            # Compute accuracy for D10\n",
        "            all_preds = np.concatenate(all_preds)\n",
        "            all_labels = np.concatenate(all_labels)\n",
        "            accuracy = np.sum(all_preds == all_labels) / len(all_labels)\n",
        "            accuracy_matrix[i - 1, i - 1] = accuracy  # Store accuracy for D10\n",
        "\n",
        "            print(f\"Accuracy for D10: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    print(\"\\nAccuracy Matrix (D1 to D10):\")\n",
        "    print(accuracy_matrix)\n",
        "\n",
        "# Call the main function to run the training and evaluation process\n",
        "train_and_evaluate()\n"
      ],
      "metadata": {
        "id": "lbR5qpU4WS62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8K6f-JTsaqQs"
      },
      "source": [
        "Final Model : using ViT for feature extraction and with a Bayesian classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "t3clh6F26nS8",
        "outputId": "49fac1e4-2898-463e-aca4-e48ac7ca1c51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Processing Dataset D1 ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-e5031634700e>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  data_dict = torch.load(file_path)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e5031634700e>\u001b[0m in \u001b[0;36m<cell line: 203>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;31m# Run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m     \u001b[0mtrain_and_evaluate_sequentially\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e5031634700e>\u001b[0m in \u001b[0;36mtrain_and_evaluate_sequentially\u001b[0;34m()\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                 \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Extract features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0mfeatures_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-e5031634700e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Use the [CLS] token embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[1;32m    637\u001b[0m         )\n\u001b[1;32m    638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 639\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    640\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    466\u001b[0m                 )\n\u001b[1;32m    467\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mlayer_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_head_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    424\u001b[0m         \u001b[0;31m# in ViT, layernorm is also applied after self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm_after\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;31m# second residual connection is done here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/vit/modeling_vit.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from transformers import ViTModel, ViTFeatureExtractor\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Dataset Class\n",
        "class CustomCIFARDataset(Dataset):\n",
        "    def __init__(self, file_path, transform=None, labeled=True):\n",
        "        try:\n",
        "            data_dict = torch.load(file_path)\n",
        "        except TypeError:\n",
        "            print(\"Warning: Falling back to non-weighted mode. Ensure the file is trusted.\")\n",
        "            data_dict = torch.load(file_path)\n",
        "\n",
        "        self.images = data_dict['data']\n",
        "        self.labels = data_dict['targets'] if labeled else None\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.images[idx]\n",
        "        image = Image.fromarray(image)  # Convert to PIL Image\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if self.labels is not None:\n",
        "            return image, self.labels[idx]\n",
        "        else:\n",
        "            return image\n",
        "\n",
        "# Feature Extractor using Vision Transformer\n",
        "class ViTFeatureExtractorModel(nn.Module):\n",
        "    def __init__(self, pretrained=True):\n",
        "        super(ViTFeatureExtractorModel, self).__init__()\n",
        "        self.vit = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "        self.feature_dim = self.vit.config.hidden_size  # Output size of ViT\n",
        "\n",
        "        # Freeze ViT weights\n",
        "        if pretrained:\n",
        "            for param in self.vit.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = self.vit(pixel_values=x, return_dict=True)\n",
        "        return outputs.last_hidden_state[:, 0, :]  # Use the [CLS] token embedding\n",
        "\n",
        "# Bayesian Classifier\n",
        "class BayesianClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(BayesianClassifier, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.input_dim = input_dim\n",
        "        self.prior_mean = nn.Parameter(torch.zeros(num_classes, input_dim), requires_grad=False)\n",
        "        self.prior_cov = nn.Parameter(torch.eye(input_dim).repeat(num_classes, 1, 1), requires_grad=False)\n",
        "\n",
        "    def update_posteriors(self, features, labels):\n",
        "        for c in range(self.num_classes):\n",
        "            class_features = features[labels == c]\n",
        "            if len(class_features) > 0:\n",
        "                n_c = len(class_features)\n",
        "                sample_mean = class_features.mean(dim=0)\n",
        "                prior_cov_inv = torch.linalg.inv(self.prior_cov[c])\n",
        "                likelihood_cov_inv = torch.eye(self.input_dim, device=features.device) * n_c\n",
        "\n",
        "                posterior_cov = torch.linalg.inv(prior_cov_inv + likelihood_cov_inv)\n",
        "                posterior_mean = posterior_cov @ (\n",
        "                    prior_cov_inv @ self.prior_mean[c] + likelihood_cov_inv @ sample_mean\n",
        "                )\n",
        "\n",
        "                self.prior_mean.data[c] = posterior_mean\n",
        "                self.prior_cov.data[c] = posterior_cov\n",
        "\n",
        "    def forward(self, features):\n",
        "        normalized_features = features / (torch.norm(features, dim=1, keepdim=True) + 1e-6)\n",
        "        normalized_prototypes = self.prior_mean / (torch.norm(self.prior_mean, dim=1, keepdim=True) + 1e-6)\n",
        "        cosine_similarity = torch.mm(normalized_features, normalized_prototypes.T)\n",
        "        return cosine_similarity\n",
        "\n",
        "# Main Functionality\n",
        "def train_and_evaluate():\n",
        "    train_data_path = '/content/drive/MyDrive/ML/dataset/dataset/part_one_dataset/train_data'\n",
        "    eval_data_path = '/content/drive/MyDrive/ML/dataset/dataset/part_one_dataset/eval_data'\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),  # Resize for ViT input\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize for ViT\n",
        "    ])\n",
        "\n",
        "    batch_size = 64\n",
        "    confidence_threshold = 0.8\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Initialize Feature Extractor\n",
        "    feature_extractor = ViTFeatureExtractorModel(pretrained=True).to(device)\n",
        "\n",
        "    accuracy_matrix = np.zeros((10, 10))\n",
        "\n",
        "    for i in range(1, 11):\n",
        "        print(f\"\\n=== Processing Dataset D{i} ===\")\n",
        "\n",
        "        train_dataset = CustomCIFARDataset(\n",
        "            os.path.join(train_data_path, f'{i}_train_data.tar.pth'),\n",
        "            transform=transform,\n",
        "            labeled=(i == 1)\n",
        "        )\n",
        "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        features_list, labels_list = [], []\n",
        "        for batch in train_loader:\n",
        "            if i == 1:  # D1 is labeled\n",
        "                images, labels = batch\n",
        "                labels_list.append(labels.to(device))\n",
        "            else:  # D2 onward\n",
        "                images = batch\n",
        "\n",
        "            images = images.to(device)\n",
        "            features = feature_extractor(images)  # Extract features\n",
        "            features_list.append(features)\n",
        "\n",
        "        features = torch.cat(features_list).to(device)  # Combine all features\n",
        "        if labels_list:\n",
        "            labels = torch.cat(labels_list).to(device)\n",
        "            num_classes = len(torch.unique(labels))\n",
        "        else:\n",
        "            labels = None\n",
        "            num_classes = 10  # Default for unlabeled datasets\n",
        "\n",
        "        # Initialize Classifier for D1\n",
        "        if i == 1:\n",
        "            classifier = BayesianClassifier(input_dim=feature_extractor.feature_dim, num_classes=num_classes).to(device)\n",
        "\n",
        "        if labels is not None:  # D1: Update posterior using true labels\n",
        "            classifier.update_posteriors(features, labels)\n",
        "        else:  # D2 onward: Pseudo-labeling and posterior update\n",
        "            with torch.no_grad():\n",
        "                logits = classifier(features)  # Predict pseudo-labels\n",
        "                predictions = logits.argmax(dim=1)\n",
        "                confidences = logits.max(dim=1).values  # Confidence scores\n",
        "\n",
        "            # Filter samples based on confidence threshold\n",
        "            high_confidence_indices = confidences > confidence_threshold\n",
        "            high_confidence_features = features[high_confidence_indices]\n",
        "            high_confidence_labels = predictions[high_confidence_indices]\n",
        "\n",
        "            # Update posterior using high-confidence pseudo-labeled samples\n",
        "            if len(high_confidence_features) > 0:\n",
        "                classifier.update_posteriors(high_confidence_features, high_confidence_labels)\n",
        "\n",
        "        print(f\"\\nEvaluating model f{i} on held-out datasets D̂1 to D̂{i}\")\n",
        "        for j in range(1, i + 1):\n",
        "            eval_dataset = CustomCIFARDataset(\n",
        "                os.path.join(eval_data_path, f'{j}_eval_data.tar.pth'),\n",
        "                transform=transform,\n",
        "                labeled=True\n",
        "            )\n",
        "            eval_loader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "            correct, total = 0, 0\n",
        "            with torch.no_grad():\n",
        "                for images, labels in eval_loader:\n",
        "                    images, labels = images.to(device), labels.to(device)\n",
        "                    features = feature_extractor(images)\n",
        "                    logits = classifier(features)\n",
        "                    predictions = logits.argmax(dim=1)\n",
        "                    correct += (predictions == labels).sum().item()\n",
        "                    total += labels.size(0)\n",
        "\n",
        "            accuracy = 100 * correct / total\n",
        "            accuracy_matrix[i - 1, j - 1] = accuracy\n",
        "            print(f\"Accuracy of model f{i} on D̂{j}: {accuracy:.2f}%\")\n",
        "\n",
        "    print(\"\\nAccuracy Matrix:\")\n",
        "    for row in accuracy_matrix:\n",
        "        print(\" \".join(f\"{val:6.2f}\" for val in row))\n",
        "\n",
        "# Run\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}